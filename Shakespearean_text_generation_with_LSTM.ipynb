{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sp-TyMxab93o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CCZoJzBsxmk",
        "outputId": "a833d6df-75ef-4628-decb-aa8a3ccdc6c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kLjc7w23tau0"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/shakespeare.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pg7B7xcJtw4l"
      },
      "outputs": [],
      "source": [
        "text = open(path, 'r').read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oKC_j_2RElE",
        "outputId": "d0739db5-7660-4478-9023-a3f5b2254571"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:\n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep sunken eyes,\n",
            "  Were an all-e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fi9CPAm-tw6S"
      },
      "outputs": [],
      "source": [
        "vocabulary = sorted(set(text))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epMHBFpURIb5",
        "outputId": "0ba3a891-44c1-4a6d-c918-5dad48fe33fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHYISbjVRP-f",
        "outputId": "a7f56bec-5ada-4910-9714-3b8b8e45b52a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5QMDSYMnuFlg"
      },
      "outputs": [],
      "source": [
        "char_to_index = {char:index for index, char in enumerate(vocabulary)}\n",
        "index_to_char = np.array(vocabulary)\n",
        "encoded_text = np.array([char_to_index[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l0E69nCuFnk",
        "outputId": "c2312c4b-f61e-4d6a-d66c-edd386d31e52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "sample_line = 'From fairest creatures we desire increase'\n",
        "len(sample_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEC133RKuFqa",
        "outputId": "6c8de5fb-1f3d-4079-d2d0-d23557da638b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "133"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "sample_three_lines = '''\n",
        "From fairest creatures we desire increase,\n",
        "  That thereby beauty's rose might never die,\n",
        "  But as the riper should by time decease,\n",
        "'''\n",
        "len(sample_three_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CCnstzp9umDh"
      },
      "outputs": [],
      "source": [
        "seq_length = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aZJh0qH5uFsa"
      },
      "outputs": [],
      "source": [
        "encoded_text = np.array([char_to_index[c] for c in text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN6i5uQruFuh",
        "outputId": "b0d9489d-0e96-4fa9-ffca-4e8c3521c82b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  1, 31, ..., 39, 29,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCT7z6RyuFwE",
        "outputId": "c27c830e-4a11-484b-d8f2-71330507ac81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75\n",
            "  1 56 74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1\n",
            " 75 64 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59\n",
            " 60 73  1 63 60 64 73  1 68 64 62 63 75  1 57 60 56 73  1 63 64 74  1 68\n",
            " 60 68 70 73 80 21  0  1  1 27 76 75  1 75 63 70 76  1 58 70 69 75 73 56\n",
            " 58 75 60 59  1 75 70  1 75 63 64 69 60  1 70 78 69  1 57 73 64 62 63 75\n",
            "  1 60 80 60 74  8  0  1  1 31 60 60 59  5 74 75  1 75 63 80  1 67 64 62\n",
            " 63 75  5 74  1 61 67 56 68 60  1 78 64 75 63  1 74 60 67 61  9 74 76 57\n",
            " 74 75 56 69 75 64 56 67  1 61 76 60 67  8  0  1  1 38 56 66 64 69 62  1\n",
            " 56  1 61 56 68 64 69 60  1 78 63 60 73 60  1 56 57 76 69 59 56 69 58 60\n",
            "  1 67 64 60 74  8  0  1  1 45 63 80  1 74 60 67 61  1 75 63 80  1 61 70\n",
            " 60  8  1 75 70  1 75 63 80  1 74 78 60 60 75  1 74 60 67 61  1 75 70 70\n",
            "  1 58 73 76 60 67 21  0  1  1 45 63 70 76  1 75 63 56 75  1 56 73 75  1\n",
            " 69 70 78  1 75 63 60  1 78 70 73 67 59  5 74  1 61 73 60 74 63  1 70 73\n",
            " 69 56 68 60 69 75  8  0  1  1 26 69 59  1 70 69 67 80  1 63 60 73 56 67\n",
            " 59  1 75 70  1 75 63 60  1 62 56 76 59 80  1 74 71 73 64 69 62  8  0  1\n",
            "  1 48 64 75 63 64 69  1 75 63 64 69 60  1 70 78 69  1 57 76 59  1 57 76\n",
            " 73 64 60 74 75  1 75 63 80  1 58 70 69 75 60 69 75  8  0  1  1 26 69 59\n",
            "  1 75 60 69 59 60 73  1 58 63 76 73 67  1 68 56 66  5 74 75  1 78 56 74\n",
            " 75 60  1 64 69  1 69 64 62 62 56 73 59 64 69 62 21  0  1  1  1  1 41 64\n",
            " 75 80  1 75 63 60  1 78 70 73 67 59  8  1 70 73  1 60 67 74 60  1 75 63\n",
            " 64 74  1 62 67 76 75 75 70 69  1 57 60  8  0  1  1  1  1 45 70  1 60 56\n",
            " 75  1 75 63 60  1 78 70 73 67 59  5 74  1 59 76 60  8  1 57 80  1 75 63\n",
            " 60  1 62 73 56 77 60  1 56 69 59  1 75 63 60 60 10  0  0  0  1  1  1  1\n",
            "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 13  0  1  1 48 63 60\n",
            " 69  1 61 70 73 75 80  1 78 64 69 75 60 73 74  1 74 63 56 67 67  1 57 60\n",
            " 74 64 60 62 60  1 75 63 80  1 57 73 70 78  8  0  1  1 26 69 59  1 59 64\n",
            " 62  1 59 60 60 71  1 75 73 60 69 58 63 60 74  1 64 69  1 75 63 80  1 57\n",
            " 60 56 76 75 80  5 74  1 61 64 60 67 59  8  0  1  1 45 63 80  1 80 70 76\n",
            " 75 63  5 74  1 71 73 70 76 59  1 67 64 77 60 73 80  1 74 70  1 62 56 81\n",
            " 60 59  1 70 69  1 69 70 78  8  0  1  1 48 64 67 67  1 57 60  1 56  1 75\n",
            " 56 75 75 60 73 60 59  1 78 60 60 59  1 70 61  1 74 68 56 67 67  1 78 70\n",
            " 73 75 63  1 63 60 67 59 21  0  1  1 45 63 60 69  1 57 60 64 69 62  1 56\n",
            " 74 66 60 59  8  1 78 63 60 73 60  1 56 67 67  1 75 63 80  1 57 60 56 76\n",
            " 75 80  1 67 64 60 74  8  0  1  1 48 63 60 73 60  1 56 67 67  1 75 63 60\n",
            "  1 75 73 60 56 74 76 73 60  1 70 61  1 75 63 80  1 67 76 74 75 80  1 59\n",
            " 56 80 74 22  0  1  1 45 70  1 74 56 80  1 78 64 75 63 64 69  1 75 63 64\n",
            " 69 60  1 70 78 69  1 59 60 60 71  1 74 76 69 66 60 69  1 60 80 60 74  8\n",
            "  0  1  1 48 60 73 60  1 56 69  1 56 67 67  9 60]\n"
          ]
        }
      ],
      "source": [
        "print(encoded_text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "quH23zVEuaBm"
      },
      "outputs": [],
      "source": [
        "def create_seq_target(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "klPcNDHJuaDN"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocabulary)\n",
        "embed_dim = 64\n",
        "lstm_units = 1024\n",
        "batch_size = 128\n",
        "buffer_size = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8I_m8pC5uaHT"
      },
      "outputs": [],
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "dataset = sequences.map(create_seq_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGoR2VER3Geh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fn71TIMpuo0V"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(batch_input_shape=[batch_size, None]),\n",
        "    Embedding(vocab_size, embed_dim),\n",
        "    LSTM(lstm_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    Dense(vocab_size)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=lambda y_true, y_pred: sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))"
      ],
      "metadata": {
        "id": "T_cjTHvz0jzV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1jBcPWVupC7",
        "outputId": "ac6d04f1-bc6d-436d-cb26-c9d7d6e6cbda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 64)           5376      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (128, None, 1024)         4460544   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 84)           86100     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4552020 (17.36 MB)\n",
            "Trainable params: 4552020 (17.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)"
      ],
      "metadata": {
        "id": "772Uo8so1wEG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPr4O6kW2Bed",
        "outputId": "8bed5667-e09f-4719-9236-74547b11fb08"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([128, 120, 84])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "metadata": {
        "id": "0DN4U6oe2HJc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = tf.squeeze(sample, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "e8XQ57R-2HGX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char[sample]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWNhmjQN2HDi",
        "outputId": "4059461d-355b-411b-e171-6a1745840863"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['S', 'c', '<', 'F', 'S', ')', 'B', 'Z', 'c', '.', 'F', '?', 'F',\n",
              "       ' ', '\\n', 'P', 'E', '_', 'M', ';', 'm', 'p', 'n', 'm', 'J', 'y',\n",
              "       'z', 'I', 'i', '8', 'g', ',', '&', 'i', '2', '`', 'b', 'a', '[',\n",
              "       \"'\", '[', 'Y', 'E', 'l', 'Y', 'J', 'i', 'I', 'V', 'm', ')', '5',\n",
              "       'g', 'L', 'X', 'I', 'Q', 'u', 's', 'b', ';', '>', '`', 'U', 'A',\n",
              "       '|', ')', 'U', '4', ']', 'Z', 'o', 'E', 'Q', 'e', 'q', 'z', 'u',\n",
              "       '7', 'v', 'H', 'w', 'a', '!', 's', 'e', 'c', '(', ']', 'I', 'C',\n",
              "       'D', 'H', 'O', '8', 'q', 't', 'A', 'B', ')', 'R', '.', '2', '-',\n",
              "       'r', '|', 'p', ',', '>', 'N', 'O', 'Q', ':', ';', 'E', '3', 'y',\n",
              "       'r', ',', 'D'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOcPSW-4upJB",
        "outputId": "466f954d-c3b7-4b9e-d567-2b5d905247e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "351/351 [==============================] - 55s 140ms/step - loss: 2.4358\n",
            "Epoch 2/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.6624\n",
            "Epoch 3/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.4356\n",
            "Epoch 4/20\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.3272\n",
            "Epoch 5/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.2638\n",
            "Epoch 6/20\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.2218\n",
            "Epoch 7/20\n",
            "351/351 [==============================] - 52s 139ms/step - loss: 1.1905\n",
            "Epoch 8/20\n",
            "351/351 [==============================] - 52s 138ms/step - loss: 1.1655\n",
            "Epoch 9/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.1444\n",
            "Epoch 10/20\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.1252\n",
            "Epoch 11/20\n",
            "351/351 [==============================] - 51s 140ms/step - loss: 1.1086\n",
            "Epoch 12/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0920\n",
            "Epoch 13/20\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0777\n",
            "Epoch 14/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0628\n",
            "Epoch 15/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0485\n",
            "Epoch 16/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 1.0344\n",
            "Epoch 17/20\n",
            "351/351 [==============================] - 52s 139ms/step - loss: 1.0211\n",
            "Epoch 18/20\n",
            "351/351 [==============================] - 51s 137ms/step - loss: 1.0076\n",
            "Epoch 19/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 0.9954\n",
            "Epoch 20/20\n",
            "351/351 [==============================] - 51s 139ms/step - loss: 0.9835\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('shakespearean_lstm.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqky1XwzaS5G",
        "outputId": "360df6a7-f3a7-436c-9ddb-56bf5589f44d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Input(batch_input_shape=[1, None]),\n",
        "    Embedding(vocab_size, embed_dim),\n",
        "    LSTM(lstm_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "model.load_weights('shakespearean_lstm.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "e1rLS-luam8m"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX00qMSqZs7j",
        "outputId": "a6068336-f0fc-4be0-db24-4eb59fe376d0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (1, None, 64)             5376      \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (1, None, 1024)           4460544   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (1, None, 84)             86100     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4552020 (17.36 MB)\n",
            "Trainable params: 4552020 (17.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Y1qHLwG-upKU"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
        "\n",
        "    num_generate = num_generate\n",
        "\n",
        "    input_eval = [char_to_index[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    temperature = temperature\n",
        "\n",
        "    model.reset_states()\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "        predictions = predictions / temperature\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(index_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "fTB9FzvhupOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca63626a-de0e-4bdf-a813-ff487a5c8da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From fairest creatures we desire increase that love desires not\n",
            "    If Better thou shalt be pardon. We are cluck'd\n",
            "    With no harm now; the wicked pitch Richard Pembrate\n",
            "    And with his palace.\n",
            "\n",
            "                Re-enter QUEEN ELIZABETH, and oXERON\n",
            "\n",
            "  ROSALIND. Marry, admrownate!\n",
            "  CLOWN. She is dulide me most within    That plainly I have mine eyes behind,\n",
            "    Or my niech only guilty of those honour\n",
            "    Foxtood challenge.\n",
            "  OTHELLO.    Captain doing I know that.\n",
            "  CASSIO. Can you ter it?\n",
            "  DUKE. That shall be the issue. Wait on thy passion hand,\n",
            "    Whose weakness live to reason for the door\n",
            "    Thorough suspicion! Give me grievances,\n",
            "    When it begins in should be certain and\n",
            "    About the rock that knew  By you, Sibyl, asight must be to Isispantial stain'd inferrow.'\n",
            "    There's patient sport. To be open'd in Rome;\n",
            "    My advice more did hath it for them;\n",
            "    Myself, whose tears were frank'd, and flam'd as willingly, to\n",
            "    quickly discht of equal days. Borthame for Princesa kiss the Empress\n",
            "    When the bow is on the de\n"
          ]
        }
      ],
      "source": [
        "start_string = \"From fairest creatures we desire increase\"\n",
        "generated_text = generate_text(model, start_string)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "starting_string = \"To be or not to be\"\n",
        "generated_text = generate_text(model, starting_string, num_generate=500, temperature=0.5)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "60Uhq3bAqHBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da497f14-7610-4465-c4b9-7c816c080bf9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To be or not to be the worship of\n",
            "    The senseless regal officer in rest?\n",
            "    O, the present merchant's ghost of all the rest,\n",
            "    That they were wont to do thee for their ministers\n",
            "    To be depending on the world and man\n",
            "    For such a fair and fresh as strong as mine.\n",
            "    Have with her beauty lie for fear of friends,\n",
            "    It was indeed air and reverence,\n",
            "    And then he will not come.\n",
            "  Jul. O, that the more I hate thee to thy will,\n",
            "                                     [Sings]\n",
            "    'Tis pity not the hand of my \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}